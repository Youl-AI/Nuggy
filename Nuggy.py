import streamlit as st
import os
import torch
from torchvision import transforms
from PIL import Image
import numpy as np
import cv2
import io

# ---------------------------------------------------------
# ğŸ› ï¸ [ì„¤ì •] ì›¹ì—ì„œë„ ìµœê°•ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤
# ---------------------------------------------------------
MODEL_PATH = "./checkpoints/best_finetuned_model.pth"
IMG_SIZE = 1024
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# (ì„±ëŠ¥ íŒŒë¼ë¯¸í„° - ì•„ê¹Œ ë§ì¶˜ ìµœì ê°’)
NOISE_CUTOFF = 0.2      # ë°°ê²½ ë…¸ì´ì¦ˆ ì œê±° ê¸°ì¤€
MIN_AREA_RATIO = 0.001   # ì‘ì€ ë¨¼ì§€ ì œê±° ê¸°ì¤€
GAMMA = 0.5              # ì„ ëª…ë„ ë³´ì •
GUIDED_R = 4             # í„¸ ë””í…Œì¼ ë°˜ê²½
GUIDED_EPS = 1e-4        # í„¸ ë””í…Œì¼ ë¯¼ê°ë„
USE_TTA = True           # ê³ ì„±ëŠ¥ ëª¨ë“œ

# ---------------------------------------------------------
# ğŸ§© í•¨ìˆ˜ ì •ì˜ (ëª¨ë¸ ë¡œë“œ & ì•Œê³ ë¦¬ì¦˜)
# ---------------------------------------------------------

# 1. ëª¨ë¸ ë¡œë“œ (ìºì‹±ì„ í†µí•´ ì†ë„ í–¥ìƒ)
@st.cache_resource
def load_model():
    # ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root_path = os.path.join(current_dir, 'DIS')
    import sys
    if project_root_path not in sys.path:
        sys.path.append(project_root_path)

    try:
        from models.isnet import ISNetDIS
        model = ISNetDIS().to(DEVICE)
        
        if not os.path.exists(MODEL_PATH):
            return None
            
        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        model.eval()
        return model
    except Exception as e:
        return None

# 2. ê°€ì´ë””ë“œ í•„í„° (OpenCV ì§ì ‘ êµ¬í˜„)
def guided_filter(I, p, r, eps):
    ksize = (2 * r + 1, 2 * r + 1)
    mean_I = cv2.boxFilter(I, cv2.CV_32F, ksize)
    mean_p = cv2.boxFilter(p, cv2.CV_32F, ksize)
    mean_Ip = cv2.boxFilter(I * p, cv2.CV_32F, ksize)
    mean_II = cv2.boxFilter(I * I, cv2.CV_32F, ksize)

    cov_Ip = mean_Ip - mean_I * mean_p
    var_I = mean_II - mean_I * mean_I

    a = cov_Ip / (var_I + eps)
    b = mean_p - a * mean_I

    mean_a = cv2.boxFilter(a, cv2.CV_32F, ksize)
    mean_b = cv2.boxFilter(b, cv2.CV_32F, ksize)

    q = mean_a * I + mean_b
    return q

# 3. ì¶”ë¡  ì—”ì§„ (Masterpiece ë¡œì§ ì ìš©)
def run_inference(model, image):
    orig_w, orig_h = image.size
    
    transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
    ])
    
    # (A) TTA ì¶”ë¡ 
    with torch.no_grad():
        img_tensor = transform(image).unsqueeze(0).to(DEVICE)
        preds_1 = model(img_tensor)
        while isinstance(preds_1, (list, tuple)): preds_1 = preds_1[0]
        mask_1 = torch.sigmoid(preds_1)
        final_mask = mask_1

        if USE_TTA:
            img_flip = image.transpose(Image.FLIP_LEFT_RIGHT)
            img_flip_tensor = transform(img_flip).unsqueeze(0).to(DEVICE)
            preds_2 = model(img_flip_tensor)
            while isinstance(preds_2, (list, tuple)): preds_2 = preds_2[0]
            mask_2 = torch.sigmoid(preds_2)
            mask_2 = torch.flip(mask_2, dims=[3]) 
            final_mask = (mask_1 + mask_2) / 2.0

    pred_mask = final_mask.squeeze().cpu().numpy()
    if pred_mask.max() != pred_mask.min():
        pred_mask = (pred_mask - pred_mask.min()) / (pred_mask.max() - pred_mask.min())

    # (B) Guided Filter
    src_img_pil = image.resize((IMG_SIZE, IMG_SIZE)).convert("L")
    src_img = np.array(src_img_pil).astype(np.float32) / 255.0
    guidance_mask = pred_mask.astype(np.float32)
    refined_mask = guided_filter(I=src_img, p=guidance_mask, r=GUIDED_R, eps=GUIDED_EPS)
    pred_mask = refined_mask

    # (C) Island Removal (ë¨¼ì§€ ì²­ì†Œ)
    pred_mask[pred_mask < NOISE_CUTOFF] = 0.0
    temp_mask = (pred_mask * 255).astype(np.uint8)
    contours, _ = cv2.findContours(temp_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if contours:
        clean_mask = np.zeros_like(temp_mask)
        total_area = temp_mask.shape[0] * temp_mask.shape[1]
        min_area = total_area * MIN_AREA_RATIO
        for contour in contours:
            if cv2.contourArea(contour) > min_area:
                cv2.drawContours(clean_mask, [contour], -1, 255, thickness=cv2.FILLED)
        pred_mask = np.where(clean_mask > 0, pred_mask, 0.0)

    # (D) ë§ˆë¬´ë¦¬
    pred_mask = np.power(pred_mask, GAMMA)
    pred_mask[pred_mask > 0.95] = 1.0
    
    # ì´ë¯¸ì§€ ë³µì›
    pred_mask = (pred_mask * 255).astype(np.uint8)
    mask_img = Image.fromarray(pred_mask).convert("L")
    mask_img = mask_img.resize((orig_w, orig_h), resample=Image.BILINEAR)
    
    result_img = image.copy()
    result_img.putalpha(mask_img)
    
    return result_img, mask_img

# ---------------------------------------------------------
# ğŸ–¥ï¸ ì›¹ UI êµ¬ì„±
# ---------------------------------------------------------
st.set_page_config(page_title="AI ëˆ„ë¼ ë§ˆìŠ¤í„°", layout="wide")

st.title("ğŸ° AI ë°°ê²½ ì œê±°ê¸° (Masterpiece Ver.)")
st.markdown("ì‚¬ìš©ìë‹˜ì˜ Fine-tuned ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ **í„¸ë í•˜ë‚˜ê¹Œì§€ ì‚´ë¦¬ëŠ”** ê³ ì„±ëŠ¥ ë°°ê²½ ì œê±°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ê³ ê¸‰ ì„¤ì •")
    st.info(f"í˜„ì¬ ëª¨ë¸: {os.path.basename(MODEL_PATH)}")
    
    # ì‚¬ìš©ìê°€ ì§ì ‘ ì¡°ì ˆ ê°€ëŠ¥í•˜ê²Œ UI ì—°ê²°
    new_cutoff = st.slider("ë°°ê²½ ì œê±° ê°•ë„ (Noise Cutoff)", 0.0, 0.1, NOISE_CUTOFF, 0.01)
    new_gamma = st.slider("í”¼ì‚¬ì²´ ì„ ëª…ë„ (Gamma)", 0.1, 1.0, GAMMA, 0.1)
    
    # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
    NOISE_CUTOFF = new_cutoff
    GAMMA = new_gamma

# ëª¨ë¸ ë¡œë“œ
model = load_model()

if model is None:
    st.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {MODEL_PATH}")
else:
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (JPG, PNG)", type=["jpg", "png", "jpeg"])

    if uploaded_file is not None:
        image = Image.open(uploaded_file).convert("RGB")
        
        col1, col2 = st.columns(2)
        with col1:
            st.image(image, caption="ì›ë³¸ ì´ë¯¸ì§€", use_column_width=True)

        if st.button("ğŸš€ ë°°ê²½ ì œê±° ì‹œì‘!", type="primary"):
            with st.spinner("AIê°€ ë°°ê²½ì„ ì§€ìš°ëŠ” ì¤‘ì…ë‹ˆë‹¤... (TTA + Guided Filter ì ìš© ì¤‘)"):
                try:
                    result_img, mask_img = run_inference(model, image)
                    
                    with col2:
                        st.image(result_img, caption="ê²°ê³¼ ì´ë¯¸ì§€", use_column_width=True)
                    
                    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                    buf = io.BytesIO()
                    result_img.save(buf, format="PNG")
                    byte_im = buf.getvalue()
                    
                    st.success("ì‘ì—… ì™„ë£Œ!")
                    st.download_button(
                        label="ğŸ“¥ ê²°ê³¼ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (PNG)",
                        data=byte_im,
                        file_name="remove_bg_result.png",
                        mime="image/png",
                    )
                    
                    # ë§ˆìŠ¤í¬ í™•ì¸ìš© (ì•„ì½”ë””ì–¸)
                    with st.expander("ğŸ” ë§ˆìŠ¤í¬(Mask) ìì„¸íˆ ë³´ê¸°"):
                        st.image(mask_img, caption="ìƒì„±ëœ ë§ˆìŠ¤í¬", width=300)
                        
                except Exception as e:
                    st.error(f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
